{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_dataset_nuevo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JA-Bar/nlp-depression/blob/master/Glove_dataset_suicidal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPduRyT8Tre7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm0_0Fja7ykF"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "!pip install -q sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujJ3ZU8z-sCV"
      },
      "source": [
        "!pip install tensorflow-text\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-fTUThaCEPY"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM,SimpleRNN,GRU,RNN\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from official.nlp import optimization \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prsponiIS0YK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1334f1a7-baca-4b2e-a1d8-45d3def1aa5f"
      },
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "display.max_colwidth : int or None\n",
            "    The maximum width in characters of a column in the repr of\n",
            "    a pandas data structure. When the column overflows, a \"...\"\n",
            "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
            "    [default: 50] [currently: 400]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2jZ4EvFDg-7"
      },
      "source": [
        "train=pd.read_csv ('/content/gdrive/MyDrive/Clasificador/out.csv')\n",
        "dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tPr-sL9K12c"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyj-FRdTSwpf",
        "outputId": "4fd4360e-b54a-4969-b8c6-903591e479db"
      },
      "source": [
        "print(train.size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "928296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXETLy8wCMV-"
      },
      "source": [
        "place=168\n",
        "filter = train[\"text\"] != \"\"\n",
        "train = train[filter]\n",
        "train = train.dropna()\n",
        "print('text: ' + train[\"text\"][place])\n",
        "print(\"label:\" + str(train[\"classification\"][place]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyeAOhy0CUJA"
      },
      "source": [
        "#train_labels = train[[\"sadness\", \"joy\", \"love\",'anger','fear','unsolve']]\n",
        "train_labels = train[[\"non-suicide\", \"suicide\"]]\n",
        "classes=train_labels.shape\n",
        "classes=classes[1]\n",
        "print(classes)\n",
        "train_labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiqrSE93CXFu"
      },
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 10\n",
        "fig_size[1] = 8\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "train_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-WLnbOCZmX"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xecj732mCb3m"
      },
      "source": [
        "X = []\n",
        "sentences = list(train[\"text\"])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "y = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Z2leyZY1ND",
        "outputId": "b99be501-dc9f-4c7b-a9a2-9fa86e2ea168"
      },
      "source": [
        "X[1:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weird not get affect compliment come someone know irl but feel really good internet stranger',\n",
              " 'finally almost never hear bad year ever swear fuck god annoying',\n",
              " 'need helpjust help cry hard',\n",
              " 'I losthello name adam struggle year afraid past year thought suicide fear anxiety close limit quiet long scar come family feeling year ago lose aunt trigger everyday feeling hopeless lose guilty remorseful thing do life but thought like little experience life time reveal feeling family break saw cut watch get worried something portray average day make feel absolutely dreadful later find attempt survivor attempt od overdose pill attempt hanging happen blackout pill never go noose still afraid first therapy diagnose severe depression social anxiety eating disorder later transfer fucken group therapy reason make feel anxious eventually last session therapy show result daily check feeling step survey mom dad come find put feel horrible afraid anxious everyday mom absolutely amaze describe happiest ever see therapy help eventually put sertaline anti anxiety anti depression sorry forget but never finish first prescription nor ever find right type anti depressant mom think want drug take recommended pill schedule week stop take time feel bad afraid damage worry cause even everything go afraid ever relapse cut develop severe insomnia day day feel hopeless worthless question still motivation move bed keep go ask nearly every night almost break everytime please please please someone anyone help scared might something drastic shape fear anxiety idk anymore',\n",
              " 'honetly idki not know even feel like nothing nowhere feel either nothing unbearably sad ignore friend every opitunity feel like loose girlfriend hurt everyone talk not anything good behind education feel alone but first time not feeling enjoy no hope dream care nothing not family not friend not even girlfriend still love complicate not word describe would something end but know not strong brave enough know weak make sad thing push away emotion empty bad use way normal not understand people hope dream mention bad feel girlfriend but get scared would die havnt bring but talk realise not even comprehend life mean anyone know ramble probably regret post ill think take place someone bad time gun head encoage people see help instead ill probably suvive might not plus life meaningless future bleak could cure cancer something useful sorry waste time',\n",
              " 'trigger warning excuse self inflict burn know crisis line use panic attack know not healthy thing but something stupid impulse burn really need help excuse father daughter know history together year see bad but always cut ankle wrist think excuse would easy one cut work car last night not self harm long time without think usual impulse lose moment say touch something hood car still hot almost curve like pattern first forearm little side wrist inch long kind wide little deep think car excuse good one but would need say work explain burn maybe wire smooshed behind engine go fix touch engine not want self harm need able explain ',\n",
              " 'end tonight not anymore quit ',\n",
              " 'everyone want edgy make self conscious feel like not stand draw yes play guitar but honestly feel like stick past taste music rock alt metal s not really make feel unique style but see friend classmates get into rap edm hard feel like fit not feel like stand others copy style would another quirky kid cringey phase many friend say look good grunge style kind agree but hard continue not even stand edgy people wear cross wallet chain tiktoks really feel like not fit category scar people might confuse clout chaser fucking tiktok boy goddamn hate life',\n",
              " 'life year oldhello year old balding male hairline trash make matter bad head huge bipolar depression cripple social anxiety balding cherry top wear hat even room alone not stop think pop xanax day try numb pain work little bit but come crash back twice hard come not know communicate people anymore not know keep relationship use one popular kid but dad pass away feel into deep dark hole arrest numerous time rehab mental hospital name reason not kill yet mom brother not would dead long ago but get point even love support not go enough keep alive anymore either go guy kill guy go bald look like child molestor one would choose ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJSU0dKaCdHH"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24jVF6n90mLx"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QsO-tuCfO3"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkMsCXi9UBU"
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCfrvESv9VRz"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 50\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz6GXBWxBMVP"
      },
      "source": [
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx4eLO6jBMX-"
      },
      "source": [
        "print(X_train[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8wuE5-VCnTR"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/gdrive/MyDrive/Clasificador/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "#print(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_h1oWsw8Bs0"
      },
      "source": [
        "deep_inputs = Input(shape=(None,))\n",
        "print(deep_inputs)\n",
        "net = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False,mask_zero=True,name = 'embeddings')(deep_inputs)\n",
        "print(net)\n",
        "net = SimpleRNN(200, return_sequences=True,name='RNN')(net)\n",
        "net = GRU(100, return_sequences=True,name='GRU')(net)\n",
        "net = LSTM(150,name='lstm_layer')(net)\n",
        "#net = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation=None, kernel_initializer='he_uniform')(net)\n",
        "#net = tf.keras.layers.MaxPooling1D(3)(net)\n",
        "#net = tf.keras.layers.GlobalMaxPool1D()(net)\n",
        "#net = tf.keras.layers.BatchNormalization()(net)\n",
        "#net=tf.keras.layers.Dropout(0.2)(net)\n",
        "#net = Dense(25,activation=None)(net)\n",
        "#net = Dense(12,activation=None)(net)\n",
        "#net = Dense(6,activation=None)(net)\n",
        "net = Dense(classes,activation='sigmoid')(net)\n",
        "model2= Model(inputs=deep_inputs, outputs=net)\n",
        "opt = tf.optimizers.Adam(learning_rate=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mC1k14CqwZ"
      },
      "source": [
        "history = model2.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRmJslaIC6ku"
      },
      "source": [
        "score = model2.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBRy2FYfKowK"
      },
      "source": [
        "def convert2embeding (word):\n",
        "  X = []\n",
        "  sentences = list(word)\n",
        "  #print(sentences)\n",
        "  for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "  #print(X)\n",
        "  token_word=tokenizer.texts_to_sequences(X[0])\n",
        "  #print(token_word)\n",
        "  final=[]\n",
        "  for tok in token_word:\n",
        "    try:\n",
        "      final.append(tok[0])\n",
        "    except:\n",
        "      final.append(0)\n",
        "  #print(final)\n",
        "  maxlen = None\n",
        "  final_word = pad_sequences([final], padding='post', maxlen=maxlen)\n",
        "  #print(final_word)\n",
        "  return final_word\n",
        "word2='Beatiful world baby'\n",
        "x=convert2embeding ([word2])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEIXC7SiAtFB"
      },
      "source": [
        "model2.save('/content/gdrive/MyDrive/Clasificador/glove2.h5')\n",
        "new_model = keras.models.load_model('/content/gdrive/MyDrive/Clasificador/glove2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FYRfEskA1Df"
      },
      "source": [
        "new_predictions = new_model.predict(x)\n",
        "print(new_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7HVpZPcDj6B"
      },
      "source": [
        "X_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mbENCmjrEkT"
      },
      "source": [
        "ll=800\n",
        "y_train[ll]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0cxp-SFsWlw"
      },
      "source": [
        "#new_model(X_train[1])\n",
        "new_model.predict(X_train[ll])\n",
        "#np.argmax(new_model.predict(X_train[ll]),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETbULwvpDlkH"
      },
      "source": [
        "X_train.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y6FkI5IMx3a"
      },
      "source": [
        "#Many\n",
        "y='beatiful world baby'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,' ',a)\n",
        "y='hate you'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='very bored'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i was so happy back then'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i am so happy'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='is so scary'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i hate my life'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i love you'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='letÂ´s party all night!!!!'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='lets have some fun baby'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='i want to kill myself'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y='fucking bitch'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y= 'i am so sad'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)\n",
        "y= 'i used to be happy'\n",
        "x2=convert2embeding([y])\n",
        "a = model2.predict(x2)\n",
        "print(y,'---- ',a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PSynsZ2fBnj"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}